{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1381deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import graycomatrix,hog\n",
    "scaler = StandardScaler()\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "import cv2\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score  # For evaluation metrics\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "375764a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 3869/3869 [01:36<00:00, 40.06it/s]\n",
      "Validation Images: 100%|██████████| 968/968 [00:29<00:00, 33.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.7324380165289256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 3869/3869 [00:48<00:00, 80.54it/s] \n",
      "Validation Images: 100%|██████████| 968/968 [00:11<00:00, 87.16it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.753099173553719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 3870/3870 [00:46<00:00, 82.87it/s] \n",
      "Validation Images: 100%|██████████| 967/967 [00:12<00:00, 75.77it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.7611168562564633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 3870/3870 [00:48<00:00, 79.25it/s] \n",
      "Validation Images: 100%|██████████| 967/967 [00:12<00:00, 80.19it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.7590486039296794\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 3870/3870 [00:48<00:00, 79.14it/s] \n",
      "Validation Images: 100%|██████████| 967/967 [00:12<00:00, 75.91it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.7621509824198552\n",
      "Mean accuracy: 0.7535707265377286\n"
     ]
    }
   ],
   "source": [
    "tb_image_dir = 'Tuberculosis'\n",
    "non_tb_image_dir = 'Normal'\n",
    "\n",
    "tb_image_paths = glob.glob(tb_image_dir + '/*.png')  # Adjust file extension if needed\n",
    "non_tb_image_paths = glob.glob(non_tb_image_dir + '/*.png')\n",
    "\n",
    "all_image_paths = tb_image_paths + non_tb_image_paths\n",
    "labels = [1] * len(tb_image_paths) + [0] * len(non_tb_image_paths)\n",
    "\n",
    "accuracies = []\n",
    "# Perform K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=None)\n",
    "for train_index, val_index in kf.split(all_image_paths):\n",
    "    X_train_paths, X_val_paths = [all_image_paths[i] for i in train_index], [all_image_paths[i] for i in val_index]\n",
    "    y_train, y_val = [labels[i] for i in train_index], [labels[i] for i in val_index]\n",
    "    \n",
    "    # Load and preprocess training images\n",
    "    X_train = []\n",
    "    for path in tqdm(X_train_paths, desc=\"Training Images\"):\n",
    "        image = cv2.imread(path)  # Load image using OpenCV or any other library\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "        # (224, 224) is a common size for pre-trained models and feature extraction.\n",
    "        gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Extract HOG features\n",
    "        #fd = hog(gray_image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2))\n",
    "        #Extract GLCM, in this case works faster than HOG\n",
    "        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "        # Normalize the histogram\n",
    "        hist /= hist.sum()\n",
    "        # Compute GLCM-like features\n",
    "        mean = (hist * np.arange(256)).sum()\n",
    "        variance = (hist * ((np.arange(256) - mean) ** 2)).sum()\n",
    "        homogeneity = (hist / (1 + np.abs(np.arange(256) - np.arange(256)[:, None]))).sum()\n",
    "        contrast = (hist * (np.abs(np.arange(256) - np.arange(256)[:, None]) ** 2)).sum()\n",
    "        X_train.append([mean, variance, homogeneity, contrast])\n",
    "        # Append extracted features to X_train or X_val based on the loop iteration\n",
    "        # normalization\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    joblib.dump(scaler, 'scaler.pkl')\n",
    "        # Apply preprocessing steps (resizing, normalization, etc.) to the image\n",
    "        # Append preprocessed image to X_train\n",
    "        \n",
    "        # Load and preprocess validation images\n",
    "    X_val = []\n",
    "    for path in tqdm(X_val_paths, desc=\"Validation Images\"):\n",
    "        image = cv2.imread(path) \n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "        gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Extract HOG features\n",
    "        #fd = hog(gray_image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2))\n",
    "        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "\n",
    "    # Normalize the histogram\n",
    "        hist /= hist.sum()\n",
    "\n",
    "    # Compute GLCM-like features\n",
    "        mean = (hist * np.arange(256)).sum()\n",
    "        variance = (hist * ((np.arange(256) - mean) ** 2)).sum()\n",
    "        homogeneity = (hist / (1 + np.abs(np.arange(256) - np.arange(256)[:, None]))).sum()\n",
    "        contrast = (hist * (np.abs(np.arange(256) - np.arange(256)[:, None]) ** 2)).sum()\n",
    "        X_val.append([mean, variance, homogeneity, contrast])\n",
    "        # Append extracted features to X_train or X_val based on the loop iteration\n",
    "        # normalization\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    model = xgb.XGBClassifier(objective='binary:logistic', eval_metric='logloss')\n",
    "\n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Evaluate the model (using multiple metrics)\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "#     precision = precision_score(y_val, y_pred)\n",
    "#     recall = recall_score(y_val, y_pred)\n",
    "#     f1_score = f1_score(y_val, y_pred)\n",
    "#     auc_roc = roc_auc_score(y_val, y_pred)\n",
    "    \n",
    "    # Print results for this fold\n",
    "    print(f\"Accuracy for this fold: {accuracy}\")\n",
    "#     print(f\"precision for this fold: {precision}\")\n",
    "#     print(f\"recall for this fold: {recall}\")\n",
    "#     print(f\"f1_score for this fold: {f1_score}\")\n",
    "#     print(f\"auc_roc for this fold: {auc_roc}\")\n",
    "    \n",
    "    # Store results in lists for later analysis\n",
    "    accuracies.append(accuracy)\n",
    "#     precisions.append(precision)\n",
    "#     recalls.append(recall)\n",
    "#     f1_scores.append(f1_score)\n",
    "#     auc_rocs.append(auc_roc)\n",
    "\n",
    "# After the loop, assess overall performance    \n",
    "print(\"Mean accuracy:\", np.mean(accuracies))\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e457f9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_model('xgboost_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7a7dcb5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: ../unseenTB2.png, Prediction: 1, Probability: 0.9212409257888794\n"
     ]
    }
   ],
   "source": [
    "#For testing purpose only\n",
    "# Assuming model and scaler are already trained and defined\n",
    "\n",
    "# Load and preprocess a single X-ray image\n",
    "def preprocess_single_image(image_path, scaler):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Preprocess image (resize, convert to grayscale, extract HOG features, etc.)\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute GLCM-like features\n",
    "    hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "    hist /= hist.sum()\n",
    "    mean = (hist * np.arange(256)).sum()\n",
    "    variance = (hist * ((np.arange(256) - mean) ** 2)).sum()\n",
    "    homogeneity = (hist / (1 + np.abs(np.arange(256) - np.arange(256)[:, None]))).sum()\n",
    "    contrast = (hist * (np.abs(np.arange(256) - np.arange(256)[:, None]) ** 2)).sum()\n",
    "\n",
    "    # Create feature vector\n",
    "    X_processed = np.array([mean, variance, homogeneity, contrast]).reshape(1, -1)\n",
    "    # Ensure the features are in the same format as used during training\n",
    "    X_processed = scaler.transform(X_processed)  # Assuming you have a trained scaler\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "# Path to the single X-ray image you want to test\n",
    "image_path_to_test = '../unseenTB2.png'\n",
    "#unseen TB : 0.81 probability i.e prediction = 1(TB)\n",
    "#unseen normal : 0.68 probability\n",
    "\n",
    "# Preprocess the single image\n",
    "X_single_image = preprocess_single_image(image_path_to_test, scaler)\n",
    "\n",
    "# Make prediction\n",
    "y_pred_single_image = model.predict(X_single_image)\n",
    "\n",
    "# If you want probability scores for binary classification\n",
    "y_proba_single_image = model.predict_proba(X_single_image)[:, 1]\n",
    "\n",
    "# Display the prediction\n",
    "print(f\"Image: {image_path_to_test}, Prediction: {y_pred_single_image[0]}, Probability: {y_proba_single_image[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d29be92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.9630952380952381\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf2efd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
