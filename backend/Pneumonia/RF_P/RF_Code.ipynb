{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d9a069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import glob\n",
    "import numpy as np\n",
    "import joblib\n",
    "# Initialize a StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2958adde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to your TB and non-TB image directories\n",
    "pneumonia_image_dir = 'Pneumonia'\n",
    "non_pneumonia_image_dir = 'Normal'\n",
    "\n",
    "# Collect image paths\n",
    "pneumonia_image_paths = glob.glob(pneumonia_image_dir + '/*.jpeg')\n",
    "non_pneumonia_image_paths = glob.glob(non_pneumonia_image_dir + '/*.jpeg')\n",
    "\n",
    "# Combine paths and create labels\n",
    "all_image_paths = pneumonia_image_paths + non_pneumonia_image_paths\n",
    "labels = [1] * len(pneumonia_image_paths) + [0] * len(non_pneumonia_image_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8441225",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images:   0%|          | 0/4684 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 4684/4684 [07:03<00:00, 11.05it/s]\n",
      "Validation Images: 100%|██████████| 1172/1172 [01:54<00:00, 10.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.7235494880546075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 4685/4685 [04:35<00:00, 17.00it/s]\n",
      "Validation Images: 100%|██████████| 1171/1171 [01:19<00:00, 14.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.740392826643894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 4685/4685 [05:14<00:00, 14.89it/s]\n",
      "Validation Images: 100%|██████████| 1171/1171 [01:19<00:00, 14.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.7224594363791631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 4685/4685 [05:12<00:00, 14.97it/s]\n",
      "Validation Images: 100%|██████████| 1171/1171 [01:33<00:00, 12.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.7190435525192144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Images: 100%|██████████| 4685/4685 [04:03<00:00, 19.24it/s]\n",
      "Validation Images: 100%|██████████| 1171/1171 [01:16<00:00, 15.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for this fold: 0.730999146029035\n",
      "Mean accuracy: 0.7272888899251828\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store accuracies\n",
    "accuracies = []\n",
    "\n",
    "# Perform K-fold cross-validation\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=None)\n",
    "\n",
    "for train_index, val_index in kf.split(all_image_paths):\n",
    "    X_train_paths, X_val_paths = [all_image_paths[i] for i in train_index], [all_image_paths[i] for i in val_index]\n",
    "    y_train, y_val = [labels[i] for i in train_index], [labels[i] for i in val_index]\n",
    "    \n",
    "    # Load and preprocess training images\n",
    "    X_train = []\n",
    "    for path in tqdm(X_train_paths, desc=\"Training Images\"):\n",
    "        image = cv2.imread(path)\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "        gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Extract features - you can use GLCM or any other feature extraction method here\n",
    "        # For example, using GLCM:\n",
    "        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "        hist /= hist.sum()\n",
    "        mean = (hist * np.arange(256)).sum()\n",
    "        variance = (hist * ((np.arange(256) - mean) ** 2)).sum()\n",
    "        homogeneity = (hist / (1 + np.abs(np.arange(256) - np.arange(256)[:, None]))).sum()\n",
    "        contrast = (hist * (np.abs(np.arange(256) - np.arange(256)[:, None]) ** 2)).sum()\n",
    "        X_train.append([mean, variance, homogeneity, contrast])\n",
    "        \n",
    "    # Normalize and transform features\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    joblib.dump(scaler, 'scaler.pkl') \n",
    "    # Load and preprocess validation images\n",
    "    X_val = []\n",
    "    for path in tqdm(X_val_paths, desc=\"Validation Images\"):\n",
    "        image = cv2.imread(path)\n",
    "        resized_image = cv2.resize(image, (224, 224))\n",
    "        gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "        # Extract features - same as in the training set\n",
    "        hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "        hist /= hist.sum()\n",
    "        mean = (hist * np.arange(256)).sum()\n",
    "        variance = (hist * ((np.arange(256) - mean) ** 2)).sum()\n",
    "        homogeneity = (hist / (1 + np.abs(np.arange(256) - np.arange(256)[:, None]))).sum()\n",
    "        contrast = (hist * (np.abs(np.arange(256) - np.arange(256)[:, None]) ** 2)).sum()\n",
    "        X_val.append([mean, variance, homogeneity, contrast])\n",
    "\n",
    "    # Normalize and transform features\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    # Initialize Random Forest classifier\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)  # You can adjust the hyperparameters\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "\n",
    "    # Make predictions on validation data\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    \n",
    "    # Print results for this fold\n",
    "    print(f\"Accuracy for this fold: {accuracy}\")\n",
    "    \n",
    "    # Store results in the list for later analysis\n",
    "    accuracies.append(accuracy)\n",
    "\n",
    "# After the loop, assess overall performance    \n",
    "print(\"Mean accuracy:\", np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98391a36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['random_forest_model.pkl']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(model, 'random_forest_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69da48b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image: unseen_P.jpeg, Prediction: 1, Probability: 0.95\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess a single X-ray image\n",
    "def preprocess_single_image(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Preprocess image (resize, convert to grayscale, extract HOG features, etc.)\n",
    "    resized_image = cv2.resize(image, (224, 224))\n",
    "    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)\n",
    "    # Compute GLCM-like features\n",
    "    hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])\n",
    "    hist /= hist.sum()\n",
    "    mean = (hist * np.arange(256)).sum()\n",
    "    variance = (hist * ((np.arange(256) - mean) ** 2)).sum()\n",
    "    homogeneity = (hist / (1 + np.abs(np.arange(256) - np.arange(256)[:, None]))).sum()\n",
    "    contrast = (hist * (np.abs(np.arange(256) - np.arange(256)[:, None]) ** 2)).sum()\n",
    "\n",
    "    # Create feature vector\n",
    "    X_processed = np.array([mean, variance, homogeneity, contrast]).reshape(1, -1)\n",
    "    # Ensure the features are in the same format as used during training\n",
    "    X_processed = scaler.transform(X_processed)  # Assuming you have a trained scaler\n",
    "    \n",
    "    return X_processed\n",
    "\n",
    "# Path to the single X-ray image you want to test\n",
    "image_path_to_test = 'unseen_P.jpeg'\n",
    "\n",
    "# Preprocess the single image\n",
    "X_single_image = preprocess_single_image(image_path_to_test)\n",
    "\n",
    "# Make prediction\n",
    "y_pred_single_image = model.predict(X_single_image)\n",
    "\n",
    "# If you want probability scores for binary classification\n",
    "y_proba_single_image = model.predict_proba(X_single_image)[:, 1]\n",
    "\n",
    "# Display the prediction\n",
    "print(f\"Image: {image_path_to_test}, Prediction: {y_pred_single_image[0]}, Probability: {y_proba_single_image[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed428e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
